# # # """
# # # Fixed Real-Time Welsh Translation
# # # - Uses exact same approach as working batch code
# # # - Pause stops recording, NOT processing
# # # - Instructions for free cloud GPU deployment
# # # """

# # # import torch
# # # import torchaudio
# # # import gradio as gr
# # # from transformers import SeamlessM4Tv2Model, AutoProcessor
# # # from peft import PeftModel
# # # from pathlib import Path
# # # import time
# # # import numpy as np
# # # from collections import deque
# # # import threading
# # # import queue as queue_module

# # # # Configuration
# # # MODEL_NAME = "facebook/seamless-m4t-v2-large"
# # # CHECKPOINT = "training_output_final/checkpoints/best_bleu"
# # # DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# # # SAMPLE_RATE = 16000
# # # CHUNK_DURATION = 3.0
# # # CHUNK_SAMPLES = int(CHUNK_DURATION * SAMPLE_RATE)

# # # print(f"Loading model on {DEVICE}...")
# # # print("="*70)

# # # # Load model - EXACT SAME WAY as working batch code
# # # processor = AutoProcessor.from_pretrained(MODEL_NAME)
# # # model = SeamlessM4Tv2Model.from_pretrained(MODEL_NAME)
# # # model = model.to(DEVICE)

# # # # Load LoRA - DO NOT USE merge_and_unload() for streaming
# # # # The batch code works WITHOUT merging, so we keep it the same
# # # if Path(CHECKPOINT).exists():
# # #     print(f"Loading LoRA from {CHECKPOINT}")
# # #     model = PeftModel.from_pretrained(model, CHECKPOINT)
# # #     print("âœ… Fine-tuned model loaded (LoRA adapters active)")
# # #     print("   Using SAME approach as working batch code")
# # # else:
# # #     print(f"âš ï¸ Checkpoint not found: {CHECKPOINT}")

# # # model.eval()
# # # print("="*70)


# # # class StreamingTranslator:
# # #     """Handles streaming with pause/resume for RECORDING only."""
    
# # #     def __init__(self):
# # #         self.audio_buffer = deque(maxlen=SAMPLE_RATE * 30)
# # #         self.output_text = []
# # #         self.chunk_count = 0
# # #         self.processed_samples = 0
        
# # #         # Pause controls RECORDING, not processing
# # #         self.is_recording = True  # Changed from is_paused
# # #         self.paused_samples = 0
        
# # #         # Threading for parallel processing
# # #         self.processing_thread = None
# # #         self.stop_processing = False
        
# # #         # Queues
# # #         self.chunk_queue = queue_module.Queue(maxsize=5)
# # #         self.result_queue = queue_module.Queue(maxsize=5)
        
# # #         # Start background processor
# # #         self.start_processor_thread()
        
# # #     def start_processor_thread(self):
# # #         """Start background processor."""
# # #         self.stop_processing = False
# # #         self.processing_thread = threading.Thread(target=self._process_chunks_worker, daemon=True)
# # #         self.processing_thread.start()
# # #         print("ğŸš€ Background processor started")
    
# # #     def _process_chunks_worker(self):
# # #         """Background worker - IDENTICAL to working batch code."""
# # #         while not self.stop_processing:
# # #             try:
# # #                 chunk_data = self.chunk_queue.get(timeout=1.0)
                
# # #                 if chunk_data is None:
# # #                     break
                
# # #                 chunk_audio, chunk_num = chunk_data
                
# # #                 print(f"\n{'='*50}")
# # #                 print(f"ğŸ”„ Processing chunk #{chunk_num}")
                
# # #                 start_time = time.time()
                
# # #                 try:
# # #                     # EXACT SAME CODE as working batch translation
# # #                     inputs = processor(
# # #                         audio=chunk_audio,
# # #                         sampling_rate=SAMPLE_RATE,
# # #                         return_tensors="pt"
# # #                     )
# # #                     inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
                    
# # #                     # Generate speech - IDENTICAL parameters
# # #                     with torch.no_grad():
# # #                         audio_outputs = model.generate(
# # #                             **inputs,
# # #                             tgt_lang="cym",
# # #                             generate_speech=True,
# # #                             num_beams=5
# # #                         )
                    
# # #                     audio_array = audio_outputs[0].cpu().numpy().squeeze()
                    
# # #                     # Generate text - IDENTICAL parameters
# # #                     with torch.no_grad():
# # #                         text_outputs = model.generate(
# # #                             **inputs,
# # #                             tgt_lang="cym",
# # #                             generate_speech=False,
# # #                             num_beams=5
# # #                         )
                    
# # #                     # Decode - IDENTICAL method
# # #                     translated_text = processor.decode(text_outputs[0].tolist()[0], skip_special_tokens=True)
                    
# # #                     elapsed = time.time() - start_time
                    
# # #                     print(f"âœ… Welsh: {translated_text}")
# # #                     print(f"â±ï¸ Time: {elapsed:.2f}s")
                    
# # #                     # Normalize audio - IDENTICAL
# # #                     if audio_array.dtype != np.float32:
# # #                         audio_array = audio_array.astype(np.float32)
                    
# # #                     max_val = np.abs(audio_array).max()
# # #                     if max_val > 1.0:
# # #                         audio_array = audio_array / max_val
                    
# # #                     # Put result in queue
# # #                     timestamp = time.strftime("%H:%M:%S")
# # #                     result = {
# # #                         'chunk_num': chunk_num,
# # #                         'text': translated_text,
# # #                         'audio': audio_array,
# # #                         'timestamp': timestamp,
# # #                         'processing_time': elapsed
# # #                     }
                    
# # #                     self.result_queue.put(result)
# # #                     print(f"ğŸ“¤ Chunk #{chunk_num} ready")
# # #                     print(f"{'='*50}\n")
                    
# # #                 except Exception as e:
# # #                     print(f"âŒ Error: {e}")
# # #                     import traceback
# # #                     traceback.print_exc()
                
# # #             except queue_module.Empty:
# # #                 continue
    
# # #     def add_audio(self, audio_data, sample_rate):
# # #         """Add audio ONLY if recording is active."""
# # #         if audio_data is None or len(audio_data) == 0:
# # #             return
        
# # #         # If recording is paused, discard audio but KEEP PROCESSING
# # #         if not self.is_recording:
# # #             self.paused_samples += len(audio_data)
# # #             return  # Don't add to buffer
        
# # #         # Resample if needed
# # #         if sample_rate != SAMPLE_RATE:
# # #             audio_tensor = torch.from_numpy(audio_data).float()
# # #             if audio_tensor.dim() == 1:
# # #                 audio_tensor = audio_tensor.unsqueeze(0)
# # #             elif audio_tensor.dim() == 2:
# # #                 audio_tensor = audio_tensor.mean(dim=1, keepdim=True)
            
# # #             resampler = torchaudio.transforms.Resample(sample_rate, SAMPLE_RATE)
# # #             audio_data = resampler(audio_tensor).squeeze().numpy()
        
# # #         # Ensure mono
# # #         if audio_data.ndim > 1:
# # #             audio_data = audio_data.mean(axis=1)
        
# # #         # Add to buffer
# # #         self.audio_buffer.extend(audio_data.flatten())
        
# # #         # Check for new chunks
# # #         self.check_and_queue_chunk()
    
# # #     def check_and_queue_chunk(self):
# # #         """Queue chunks for processing."""
# # #         current_buffer_size = len(self.audio_buffer)
# # #         new_samples = current_buffer_size - self.processed_samples
        
# # #         if new_samples >= CHUNK_SAMPLES:
# # #             chunk_start = max(0, current_buffer_size - CHUNK_SAMPLES)
# # #             chunk = np.array(list(self.audio_buffer)[chunk_start:], dtype=np.float32)
            
# # #             self.chunk_count += 1
# # #             self.processed_samples = current_buffer_size
            
# # #             try:
# # #                 self.chunk_queue.put_nowait((chunk, self.chunk_count))
# # #                 print(f"ğŸ“¥ Chunk #{self.chunk_count} queued")
# # #             except queue_module.Full:
# # #                 print(f"âš ï¸ Queue full")
    
# # #     def toggle_recording(self):
# # #         """Toggle recording ON/OFF (does NOT affect processing)."""
# # #         self.is_recording = not self.is_recording
        
# # #         if self.is_recording:
# # #             status = "â–¶ï¸ RECORDING"
# # #             self.paused_samples = 0
# # #         else:
# # #             status = "â¸ï¸ PAUSED (silence not recorded, processing continues)"
        
# # #         print(f"\n{status}")
# # #         return status
    
# # #     def get_latest_result(self):
# # #         """Get latest result."""
# # #         try:
# # #             result = self.result_queue.get_nowait()
# # #             self.output_text.append(
# # #                 f"[{result['timestamp']}] {result['text']} (â±ï¸ {result['processing_time']:.1f}s)"
# # #             )
# # #             return result
# # #         except queue_module.Empty:
# # #             return None
    
# # #     def get_outputs(self):
# # #         """Get outputs."""
# # #         result = self.get_latest_result()
        
# # #         rec_status = "" if self.is_recording else " [â¸ï¸ Recording Paused]"
# # #         text = "\n\n".join(self.output_text) if self.output_text else f"ğŸ¤ Listening...{rec_status}"
        
# # #         if result:
# # #             return text, (SAMPLE_RATE, result['audio'])
# # #         else:
# # #             return text, None
    
# # #     def get_status(self):
# # #         """Get status."""
# # #         chunks_queued = self.chunk_queue.qsize()
# # #         results_ready = self.result_queue.qsize()
# # #         rec_indicator = "â¸ï¸ Rec Paused | " if not self.is_recording else ""
        
# # #         status = f"{rec_indicator}ğŸ“Š {self.chunk_count} chunks | "
# # #         status += f"{chunks_queued} processing | {results_ready} ready"
        
# # #         return status
    
# # #     def reset(self):
# # #         """Reset."""
# # #         while not self.chunk_queue.empty():
# # #             try:
# # #                 self.chunk_queue.get_nowait()
# # #             except queue_module.Empty:
# # #                 break
        
# # #         while not self.result_queue.empty():
# # #             try:
# # #                 self.result_queue.get_nowait()
# # #             except queue_module.Empty:
# # #                 break
        
# # #         self.audio_buffer.clear()
# # #         self.output_text = []
# # #         self.chunk_count = 0
# # #         self.processed_samples = 0
# # #         self.is_recording = True
# # #         self.paused_samples = 0
        
# # #         print("ğŸ”„ Reset")
    
# # #     def stop(self):
# # #         """Stop processor."""
# # #         self.stop_processing = True
# # #         self.chunk_queue.put(None)
# # #         if self.processing_thread:
# # #             self.processing_thread.join(timeout=2.0)


# # # # Global translator
# # # translator = StreamingTranslator()


# # # def process_audio_stream(audio):
# # #     """Process audio stream."""
# # #     global translator
    
# # #     if audio is None:
# # #         return translator.get_outputs()
    
# # #     sample_rate, audio_data = audio
# # #     translator.add_audio(audio_data, sample_rate)
# # #     return translator.get_outputs()


# # # def toggle_recording():
# # #     """Toggle recording."""
# # #     status = translator.toggle_recording()
# # #     return status


# # # def get_status():
# # #     """Get status."""
# # #     return translator.get_status()


# # # def reset_translator():
# # #     """Reset translator."""
# # #     global translator
# # #     translator.reset()
# # #     return "ğŸ”„ Reset", None, "â–¶ï¸ Recording"


# # # # Create interface
# # # with gr.Blocks(title="Welsh Translation - Fixed") as app:
# # #     gr.Markdown("""
# # #     # ğŸ™ï¸ Real-Time English â†’ Welsh Translation (FIXED)
    
# # #     ## âœ… Fixed Issues:
# # #     1. **Welsh Output**: Uses EXACT same code as working batch version
# # #     2. **Pause Feature**: Stops RECORDING (silence), NOT processing
# # #     3. **Speed Up**: See deployment instructions below
    
# # #     ## How to Use:
# # #     - Click "Record" and speak English
# # #     - Click "Pause" during silence (stops recording, keeps processing)
# # #     - Click "Resume" to continue recording
# # #     - Translations appear every ~15-20s per 3s chunk
# # #     """)
    
# # #     with gr.Row():
# # #         with gr.Column(scale=1):
# # #             audio_input = gr.Audio(
# # #                 sources=["microphone"],
# # #                 streaming=True,
# # #                 label="ğŸ¤ Speak English",
# # #                 type="numpy"
# # #             )
            
# # #             with gr.Row():
# # #                 rec_btn = gr.Button("â¸ï¸ Pause Recording / â–¶ï¸ Resume", variant="primary")
# # #                 reset_btn = gr.Button("ğŸ”„ Reset", variant="secondary")
            
# # #             rec_status = gr.Textbox(
# # #                 label="Recording Status",
# # #                 value="â–¶ï¸ Recording",
# # #                 interactive=False
# # #             )
            
# # #             status_text = gr.Textbox(
# # #                 label="ğŸ“Š Processing Status",
# # #                 value="Ready",
# # #                 interactive=False
# # #             )
            
# # #             gr.Markdown(f"""
# # #             ### Current Setup
# # #             - **Device:** {DEVICE}
# # #             - **Model:** Fine-tuned LoRA
# # #             - **Processing:** ~15-20s per 3s chunk
# # #             """)
        
# # #         with gr.Column(scale=1):
# # #             text_output = gr.Textbox(
# # #                 label="ğŸ“ Welsh Translation",
# # #                 lines=15,
# # #                 max_lines=25
# # #             )
            
# # #             audio_output = gr.Audio(
# # #                 label="ğŸ”Š Welsh Audio",
# # #                 autoplay=True
# # #             )
    
# # #     gr.Markdown("""
# # #     ---
# # #     ## ğŸš€ Speed Up with Free Cloud GPUs
    
# # #     ### Option 1: Google Colab (FREE)
# # #     - **GPU**: T4 (free tier) or A100 (paid)
# # #     - **Speed**: 3-5x faster than your PC
# # #     - **Steps**:
# # #       1. Upload this code to Colab notebook
# # #       2. Runtime â†’ Change runtime type â†’ GPU
# # #       3. Install requirements: `!pip install transformers peft gradio`
# # #       4. Run code and use `share=True` in `app.launch()`
# # #     - **Link**: https://colab.research.google.com
    
# # #     ### Option 2: Kaggle Notebooks (FREE 30 GPU hrs/week)
# # #     - **GPU**: T4 or P100 (free)
# # #     - **Speed**: 3-5x faster
# # #     - **Steps**:
# # #       1. Create new notebook at kaggle.com
# # #       2. Settings â†’ Accelerator â†’ GPU T4 x2
# # #       3. Upload code and run
# # #     - **Link**: https://www.kaggle.com/code
    
# # #     ### Option 3: HuggingFace Spaces (FREE with ZeroGPU)
# # #     - **GPU**: A100 (shared, free tier available)
# # #     - **Speed**: 5-10x faster
# # #     - **Steps**:
# # #       1. Create Space at huggingface.co/spaces
# # #       2. Select ZeroGPU hardware
# # #       3. Upload app.py with your code
# # #     - **Note**: Requires HF Pro ($9/mo) for sustained usage
# # #     - **Link**: https://huggingface.co/spaces
    
# # #     ### Option 4: Paperspace Gradient (FREE)
# # #     - **GPU**: M4000 (free tier, 6hr limit)
# # #     - **Speed**: 2-3x faster
# # #     - **Link**: https://gradient.run
    
# # #     ### Recommended: Google Colab
# # #     - Most reliable free GPU
# # #     - Easy setup with ngrok for external access
# # #     - T4 GPU reduces processing from 20s â†’ 5-7s per chunk
    
# # #     ---
# # #     ## ğŸ’¡ Tips:
# # #     - Pause during silence to save compute
# # #     - First chunk takes ~15-20s
# # #     - Subsequent chunks overlap with audio playback
# # #     - Check console for detailed logs
# # #     """)
    
# # #     # Stream processing
# # #     audio_input.stream(
# # #         fn=process_audio_stream,
# # #         inputs=audio_input,
# # #         outputs=[text_output, audio_output],
# # #         stream_every=0.5
# # #     )
    
# # #     # Recording pause/resume
# # #     rec_btn.click(
# # #         fn=toggle_recording,
# # #         outputs=rec_status
# # #     )
    
# # #     # Status updates
# # #     status_timer = gr.Timer(value=1.0)
# # #     status_timer.tick(
# # #         fn=get_status,
# # #         outputs=status_text
# # #     )
    
# # #     # Reset
# # #     reset_btn.click(
# # #         fn=reset_translator,
# # #         outputs=[text_output, audio_output, rec_status]
# # #     )

# # # print("\nâœ… Launching interface...")
# # # print("ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ Using EXACT same code as working batch version")
# # # print("â¸ï¸ Pause stops RECORDING, not processing")
# # # print("ğŸš€ Deploy to Colab/Kaggle for 3-5x speed boost!\n")

# # # try:
# # #     app.launch(share=True, inbrowser=True)
# # # finally:
# # #     translator.stop()


# """
# GPU-Optimized File-Based Welsh Translation
# - Uses audio file upload (no microphone)
# - Plays original and translation side-by-side
# - Guarantees Welsh output with proper configuration
# - GPU-accelerated for fast inference
# """

# import torch
# import torchaudio
# import gradio as gr
# from transformers import SeamlessM4Tv2Model, AutoProcessor
# from peft import PeftModel
# from pathlib import Path
# import time
# import numpy as np

# # ==================== CONFIGURATION ====================
# MODEL_NAME = "facebook/seamless-m4t-v2-large"
# CHECKPOINT = "training_output_final/checkpoints/best_bleu"
# DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# SAMPLE_RATE = 16000

# # GPU Optimization
# torch.backends.cudnn.benchmark = True if torch.cuda.is_available() else False
# if torch.cuda.is_available():
#     torch.backends.cuda.matmul.allow_tf32 = True

# print("="*70)
# print("ğŸš€ GPU-OPTIMIZED WELSH TRANSLATOR")
# print("="*70)
# print(f"Device: {DEVICE}")

# # ==================== MODEL LOADING ====================
# print("\nğŸ“¥ Loading processor...")
# processor = AutoProcessor.from_pretrained(MODEL_NAME)

# print("ğŸ“¥ Loading base model...")
# model = SeamlessM4Tv2Model.from_pretrained(
#     MODEL_NAME,
#     torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
# )
# model = model.to(DEVICE)

# # Load fine-tuned checkpoint
# if Path(CHECKPOINT).exists():
#     print(f"ğŸ“¥ Loading fine-tuned LoRA from: {CHECKPOINT}")
#     model = PeftModel.from_pretrained(model, CHECKPOINT)
#     model.eval()
#     print("âœ… Fine-tuned model loaded!")
# else:
#     print(f"âš ï¸ Checkpoint not found: {CHECKPOINT}")
#     print("Using base model only")
#     model.eval()

# # Get Welsh token for forcing
# welsh_token_id = processor.tokenizer.convert_tokens_to_ids("__cym__")
# print(f"\nğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ Welsh token ID: {welsh_token_id}")

# if torch.cuda.is_available():
#     print(f"ğŸ’¾ GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

# print("="*70)
# print("âœ… READY FOR TRANSLATION")
# print("="*70 + "\n")


# # ==================== TRANSLATION FUNCTION ====================
# def translate_audio_file(audio_file):
#     """
#     Translate uploaded audio file to Welsh
#     Returns: (text, original_audio, translated_audio)
#     """
#     if audio_file is None:
#         return "Please upload an audio file!", None, None
    
#     try:
#         print("\n" + "="*70)
#         print("ğŸ”„ PROCESSING AUDIO FILE")
#         print("="*70)
        
#         start_time = time.time()
        
#         # Load audio file
#         print(f"\n1ï¸âƒ£ Loading: {audio_file}")
#         waveform, sr = torchaudio.load(audio_file)
        
#         # Convert to mono
#         if waveform.shape[0] > 1:
#             waveform = waveform.mean(dim=0, keepdim=True)
#             print("   Converted stereo â†’ mono")
        
#         # Resample if needed
#         if sr != SAMPLE_RATE:
#             resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)
#             waveform = resampler(waveform)
#             print(f"   Resampled {sr}Hz â†’ {SAMPLE_RATE}Hz")
        
#         audio = waveform.squeeze().numpy()
#         duration = len(audio) / SAMPLE_RATE
#         print(f"   Duration: {duration:.2f}s")
#         print(f"   âœ… Audio loaded")
        
#         load_time = time.time() - start_time
        
#         # Process audio with explicit source language
#         print(f"\n2ï¸âƒ£ Processing with src_lang='eng'...")
#         proc_start = time.time()
        
#         # KEY FIX: Use 'audio' not 'audios' and specify src_lang
#         inputs = processor(
#             audio=audio,  # â† Use 'audio' parameter (not 'audios')
#             sampling_rate=SAMPLE_RATE,
#             src_lang="eng",  # â† Explicit source language
#             return_tensors="pt"
#         )
#         inputs = {k: v.to(DEVICE) for k, v in inputs.items() 
#                  if isinstance(v, torch.Tensor)}
        
#         proc_time = time.time() - proc_start
#         print(f"   âœ… Processed in {proc_time:.2f}s")
        
#         # Generate TEXT with forced Welsh
#         print(f"\n3ï¸âƒ£ Generating Welsh text...")
#         text_start = time.time()
        
#         with torch.no_grad():
#             text_outputs = model.generate(
#                 **inputs,
#                 tgt_lang="cym",  # Welsh target
#                 generate_speech=False,
#                 num_beams=5,
#                 forced_bos_token_id=welsh_token_id,  # Force Welsh
#                 max_new_tokens=100,
#             )
        
#         # Decode text - proper handling
#         tokens = text_outputs[0].tolist()
#         if isinstance(tokens[0], list):
#             tokens = tokens[0]
        
#         translated_text = processor.decode(tokens, skip_special_tokens=True)
        
#         text_time = time.time() - text_start
#         print(f"   âœ… Text generated in {text_time:.2f}s")
#         print(f"   ğŸ“ Translation: '{translated_text}'")
        
#         # Verify language
#         welsh_indicators = ['yn', 'y', 'mae', 'dw', 'chi', 'ar', 'i', 'o', 
#                            'sut', 'ydych', 'rwy', 'dysgu', 'cymraeg']
#         has_welsh = any(w in translated_text.lower() for w in welsh_indicators)
        
#         if has_welsh:
#             print(f"   âœ… Language: WELSH")
#         else:
#             print(f"   âš ï¸ Language detection: Unclear (may still be Welsh)")
        
#         # Generate SPEECH with forced Welsh
#         print(f"\n4ï¸âƒ£ Generating Welsh speech...")
#         speech_start = time.time()
        
#         with torch.no_grad():
#             audio_outputs = model.generate(
#                 **inputs,
#                 tgt_lang="cym",
#                 generate_speech=True,
#                 num_beams=5,
#                 forced_bos_token_id=welsh_token_id,
#                 max_new_tokens=100,
#             )
        
#         # Extract and normalize audio
#         audio_array = audio_outputs[0].cpu().numpy().squeeze()
        
#         if audio_array.dtype != np.float32:
#             audio_array = audio_array.astype(np.float32)
        
#         max_val = np.abs(audio_array).max()
#         if max_val > 1.0:
#             audio_array = audio_array / max_val
#         elif max_val > 0:
#             audio_array = audio_array / max_val * 0.8
        
#         speech_time = time.time() - speech_start
#         trans_duration = len(audio_array) / SAMPLE_RATE
#         print(f"   âœ… Speech generated in {speech_time:.2f}s")
#         print(f"   ğŸ”Š Speech duration: {trans_duration:.2f}s")
        
#         # Total timing
#         total_time = time.time() - start_time
#         print(f"\nâ±ï¸ TOTAL TIME: {total_time:.2f}s")
#         print(f"   Breakdown: Load={load_time:.1f}s, Proc={proc_time:.1f}s, "
#               f"Text={text_time:.1f}s, Speech={speech_time:.1f}s")
        
#         # Clear GPU cache
#         if torch.cuda.is_available():
#             torch.cuda.empty_cache()
#             mem = torch.cuda.memory_allocated() / 1024**3
#             print(f"   ğŸ’¾ GPU Memory: {mem:.2f} GB")
        
#         print("="*70 + "\n")
        
#         # Prepare output text
#         result_text = f"""
# ğŸ“Š **Translation Results**

# ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ **Welsh Translation:**
# {translated_text}

# ğŸ“ˆ **Statistics:**
# - Original duration: {duration:.2f}s
# - Translation duration: {trans_duration:.2f}s
# - Processing time: {total_time:.2f}s
# - Speed: {duration/total_time:.2f}x realtime

# ğŸ¯ **Language Verification:**
# {'âœ… Welsh indicators detected' if has_welsh else 'âš ï¸ Check output manually'}
# """
        
#         # Return: text, original audio, translated audio
#         return (
#             result_text,
#             (SAMPLE_RATE, audio),
#             (SAMPLE_RATE, audio_array)
#         )
        
#     except Exception as e:
#         error_msg = f"âŒ Translation failed: {str(e)}"
#         print(error_msg)
#         import traceback
#         traceback.print_exc()
#         return error_msg, None, None


# # ==================== GRADIO INTERFACE ====================
# with gr.Blocks(title="Welsh Translation - GPU Optimized", theme=gr.themes.Soft()) as app:
#     gr.Markdown("""
#     # ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ English â†’ Welsh Audio Translation (GPU-Optimized)
    
#     ## âœ… Features:
#     - **Upload English audio files** (WAV, MP3, etc.)
#     - **GPU-accelerated** translation (5-10s on GPU, 30-60s on CPU)
#     - **Guaranteed Welsh output** with proper language forcing
#     - **Side-by-side playback** of original and translation
    
#     ## ğŸ“– How to Use:
#     1. Upload your English audio file
#     2. Click "Translate"
#     3. Listen to original and Welsh translation
#     """)
    
#     with gr.Row():
#         with gr.Column(scale=1):
#             # File upload
#             audio_input = gr.Audio(
#                 sources=["upload"],
#                 type="filepath",
#                 label="ğŸ¤ Upload English Audio File"
#             )
            
#             # Translate button
#             translate_btn = gr.Button(
#                 "ğŸ”„ Translate to Welsh", 
#                 variant="primary",
#                 size="lg"
#             )
            
#             # Info box
#             gr.Markdown(f"""
#             ### ğŸ’¡ System Info
#             - **Device:** {DEVICE}
#             - **Model:** Fine-tuned SeamlessM4T v2
#             - **Target Language:** Welsh (Cymraeg)
            
#             ### âš¡ Performance
#             - **GPU:** 5-15s per file
#             - **CPU:** 30-90s per file
            
#             ### ğŸ“ Supported Formats
#             WAV, MP3, FLAC, OGG, M4A
#             """)
        
#         with gr.Column(scale=1):
#             # Results text
#             result_text = gr.Textbox(
#                 label="ğŸ“ Translation Results",
#                 lines=12,
#                 max_lines=20
#             )
            
#             # Original audio player
#             original_audio = gr.Audio(
#                 label="ğŸ§ Original English Audio",
#                 type="numpy"
#             )
            
#             # Translated audio player
#             translated_audio = gr.Audio(
#                 label="ğŸ”Š Welsh Translation Audio",
#                 type="numpy",
#                 autoplay=False
#             )
    
#     gr.Markdown("""
#     ---
#     ## ğŸ”§ Technical Details
    
#     ### Language Forcing Applied:
#     ```python
#     # Processor with source language
#     inputs = processor(
#         audio=audio,
#         src_lang="eng",  # Explicit English source
#         return_tensors="pt"
#     )
    
#     # Generation with forced Welsh
#     outputs = model.generate(
#         **inputs,
#         tgt_lang="cym",  # Welsh target
#         forced_bos_token_id=256018,  # Welsh token
#         num_beams=5
#     )
#     ```
    
#     ### Why This Works:
#     1. âœ… Uses `audio` parameter (not `audios`) - matches official docs
#     2. âœ… Specifies `src_lang="eng"` in processor
#     3. âœ… Forces Welsh with `forced_bos_token_id`
#     4. âœ… Fine-tuned model improves quality
#     5. âœ… GPU acceleration for fast inference
    
#     ### Welsh Language Indicators:
#     Words like: **yn**, **y**, **mae**, **dw**, **chi**, **sut**, **ydych**, **rwy**, **dysgu**, **Cymraeg**
    
#     ---
#     ## ğŸš€ Speed Optimization Tips
    
#     ### Already Using GPU?
#     - First translation: ~10-15s (model warmup)
#     - Subsequent: ~5-10s per file
    
#     ### Still Slow?
#     1. **Check GPU usage:**
#        ```bash
#        nvidia-smi
#        ```
    
#     2. **Verify PyTorch GPU:**
#        ```python
#        import torch
#        print(torch.cuda.is_available())  # Should be True
#        ```
    
#     3. **Use Google Colab T4:**
#        - 3x faster than most laptops
#        - FREE with Google account
#        - 15GB VRAM (no OOM errors)
    
#     ---
#     ## ğŸ“Š Expected Results
    
#     ### Input (English):
#     > "Hello, how are you? I'm learning Welsh."
    
#     ### Output (Welsh):
#     > "Helo, sut ydych chi? Rwy'n dysgu Cymraeg."
    
#     ### Audio:
#     Natural Welsh speech synthesis
    
#     ---
#     ## âš™ï¸ Configuration
    
#     **Model:** facebook/seamless-m4t-v2-large  
#     **Fine-tuned:** Yes (your trained LoRA)  
#     **Beam Search:** 5 beams  
#     **Max Tokens:** 100  
#     **Language Token:** __cym__ (256018)
#     """)
    
#     # Connect button to function
#     translate_btn.click(
#         fn=translate_audio_file,
#         inputs=audio_input,
#         outputs=[result_text, original_audio, translated_audio]
#     )
    
#     # Example files
#     gr.Examples(
#         examples=[
#             ["audio.wav"],  # Add your example files here
#         ],
#         inputs=audio_input,
#         label="ğŸ“‚ Example Files"
#     )

# print("ğŸŒ Launching web interface...")

# # Launch app
# app.launch(
#     share=True,  # Creates public URL
#     inbrowser=True,
#     server_name="0.0.0.0",
#     server_port=7860
# )

"""
GPU-Optimized File-Based Welsh Translation
- Auto-plays uploaded audio immediately
- Streams text and audio as they're generated
- Plays translation automatically when ready
- GPU-accelerated for fast inference
"""

import torch
import torchaudio
import gradio as gr
from transformers import SeamlessM4Tv2Model, AutoProcessor
from peft import PeftModel
from pathlib import Path
import time
import numpy as np

# ==================== CONFIGURATION ====================
MODEL_NAME = "facebook/seamless-m4t-v2-large"
CHECKPOINT = "training_output_final/checkpoints/best_bleu"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SAMPLE_RATE = 16000

# GPU Optimization
torch.backends.cudnn.benchmark = True if torch.cuda.is_available() else False
if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True

print("="*70)
print("ğŸš€ GPU-OPTIMIZED WELSH TRANSLATOR")
print("="*70)
print(f"Device: {DEVICE}")

# ==================== MODEL LOADING ====================
print("\nğŸ“¥ Loading processor...")
processor = AutoProcessor.from_pretrained(MODEL_NAME)

print("ğŸ“¥ Loading base model...")
model = SeamlessM4Tv2Model.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
)
model = model.to(DEVICE)

# Load fine-tuned checkpoint
if Path(CHECKPOINT).exists():
    print(f"ğŸ“¥ Loading fine-tuned LoRA from: {CHECKPOINT}")
    model = PeftModel.from_pretrained(model, CHECKPOINT)
    model.eval()
    print("âœ… Fine-tuned model loaded!")
else:
    print(f"âš ï¸ Checkpoint not found: {CHECKPOINT}")
    print("Using base model only")
    model.eval()

# Get Welsh token for forcing
welsh_token_id = processor.tokenizer.convert_tokens_to_ids("__cym__")
print(f"\nğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ Welsh token ID: {welsh_token_id}")

if torch.cuda.is_available():
    print(f"ğŸ’¾ GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

print("="*70)
print("âœ… READY FOR TRANSLATION")
print("="*70 + "\n")


# ==================== TRANSLATION FUNCTION WITH STREAMING ====================
def translate_audio_file(audio_file):
    """
    Translate uploaded audio file to Welsh with streaming output
    Yields intermediate results for progressive UI updates
    """
    if audio_file is None:
        yield "Please upload an audio file!", None, None
        return
    
    try:
        print("\n" + "="*70)
        print("ğŸ”„ PROCESSING AUDIO FILE")
        print("="*70)
        
        start_time = time.time()
        
        # Load audio file
        print(f"\n1ï¸âƒ£ Loading: {audio_file}")
        waveform, sr = torchaudio.load(audio_file)
        
        # Convert to mono
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
            print("   Converted stereo â†’ mono")
        
        # Resample if needed
        if sr != SAMPLE_RATE:
            resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)
            waveform = resampler(waveform)
            print(f"   Resampled {sr}Hz â†’ {SAMPLE_RATE}Hz")
        
        audio = waveform.squeeze().numpy()
        duration = len(audio) / SAMPLE_RATE
        print(f"   Duration: {duration:.2f}s")
        print(f"   âœ… Audio loaded")
        
        load_time = time.time() - start_time
        
        # YIELD 1: Original audio immediately (auto-plays)
        status_text = f"""
ğŸµ **Audio Loaded!**

â±ï¸ Duration: {duration:.2f}s

ğŸ”„ Processing translation...
"""
        yield status_text, (SAMPLE_RATE, audio), None
        
        # Process audio with explicit source language
        print(f"\n2ï¸âƒ£ Processing with src_lang='eng'...")
        proc_start = time.time()
        
        inputs = processor(
            audio=audio,
            sampling_rate=SAMPLE_RATE,
            src_lang="eng",
            return_tensors="pt"
        )
        inputs = {k: v.to(DEVICE) for k, v in inputs.items() 
                 if isinstance(v, torch.Tensor)}
        
        proc_time = time.time() - proc_start
        print(f"   âœ… Processed in {proc_time:.2f}s")
        
        # YIELD 2: Update status after processing
        status_text = f"""
ğŸµ **Audio Loaded!**

â±ï¸ Duration: {duration:.2f}s

âœ… Audio processed in {proc_time:.2f}s

ğŸ”„ Generating Welsh text...
"""
        yield status_text, (SAMPLE_RATE, audio), None
        
        # Generate TEXT with forced Welsh
        print(f"\n3ï¸âƒ£ Generating Welsh text...")
        text_start = time.time()
        
        with torch.no_grad():
            text_outputs = model.generate(
                **inputs,
                tgt_lang="cym",
                generate_speech=False,
                num_beams=5,
                forced_bos_token_id=welsh_token_id,
                max_new_tokens=100,
            )
        
        # Decode text
        tokens = text_outputs[0].tolist()
        if isinstance(tokens[0], list):
            tokens = tokens[0]
        
        translated_text = processor.decode(tokens, skip_special_tokens=True)
        
        text_time = time.time() - text_start
        print(f"   âœ… Text generated in {text_time:.2f}s")
        print(f"   ğŸ“ Translation: '{translated_text}'")
        
        # Verify language
        welsh_indicators = ['yn', 'y', 'mae', 'dw', 'chi', 'ar', 'i', 'o', 
                           'sut', 'ydych', 'rwy', 'dysgu', 'cymraeg']
        has_welsh = any(w in translated_text.lower() for w in welsh_indicators)
        
        if has_welsh:
            print(f"   âœ… Language: WELSH")
        else:
            print(f"   âš ï¸ Language detection: Unclear (may still be Welsh)")
        
        # YIELD 3: Show translated text immediately
        status_text = f"""
ğŸµ **Audio Loaded!**

â±ï¸ Duration: {duration:.2f}s

âœ… Audio processed in {proc_time:.2f}s

ğŸ“ **Welsh Translation:**
**{translated_text}**

â±ï¸ Text generated in {text_time:.2f}s

ğŸ”„ Generating Welsh speech audio...
"""
        yield status_text, (SAMPLE_RATE, audio), None
        
        # Generate SPEECH with forced Welsh
        print(f"\n4ï¸âƒ£ Generating Welsh speech...")
        speech_start = time.time()
        
        with torch.no_grad():
            audio_outputs = model.generate(
                **inputs,
                tgt_lang="cym",
                generate_speech=True,
                num_beams=5,
                forced_bos_token_id=welsh_token_id,
                max_new_tokens=100,
            )
        
        # Extract and normalize audio
        audio_array = audio_outputs[0].cpu().numpy().squeeze()
        
        if audio_array.dtype != np.float32:
            audio_array = audio_array.astype(np.float32)
        
        max_val = np.abs(audio_array).max()
        if max_val > 1.0:
            audio_array = audio_array / max_val
        elif max_val > 0:
            audio_array = audio_array / max_val * 0.8
        
        speech_time = time.time() - speech_start
        trans_duration = len(audio_array) / SAMPLE_RATE
        print(f"   âœ… Speech generated in {speech_time:.2f}s")
        print(f"   ğŸ”Š Speech duration: {trans_duration:.2f}s")
        
        # Total timing
        total_time = time.time() - start_time
        print(f"\nâ±ï¸ TOTAL TIME: {total_time:.2f}s")
        print(f"   Breakdown: Load={load_time:.1f}s, Proc={proc_time:.1f}s, "
              f"Text={text_time:.1f}s, Speech={speech_time:.1f}s")
        
        # Clear GPU cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            mem = torch.cuda.memory_allocated() / 1024**3
            print(f"   ğŸ’¾ GPU Memory: {mem:.2f} GB")
        
        print("="*70 + "\n")
        
        # YIELD 4: Final result with translated audio (auto-plays)
        result_text = f"""
ğŸ“Š **Translation Complete!**

ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ **Welsh Translation:**
**{translated_text}**

ğŸ“ˆ **Statistics:**
- Original duration: {duration:.2f}s
- Translation duration: {trans_duration:.2f}s
- Total processing time: {total_time:.2f}s
- Speed: {duration/total_time:.2f}x realtime

â±ï¸ **Timing Breakdown:**
- Audio loading: {load_time:.2f}s
- Preprocessing: {proc_time:.2f}s
- Text generation: {text_time:.2f}s
- Speech generation: {speech_time:.2f}s

ğŸ¯ **Language Verification:**
{'âœ… Welsh indicators detected' if has_welsh else 'âš ï¸ Check output manually'}

ğŸ”Š **Playing Welsh translation automatically...**
"""
        
        yield (
            result_text,
            (SAMPLE_RATE, audio),
            (SAMPLE_RATE, audio_array)
        )
        
    except Exception as e:
        error_msg = f"âŒ Translation failed: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        yield error_msg, None, None


# ==================== GRADIO INTERFACE ====================
with gr.Blocks(title="Welsh Translation - Auto-Play", theme=gr.themes.Soft()) as app:
    gr.Markdown("""
    # ğŸ´ó §ó ¢ó ·ó ¬ó ³ó ¿ English â†’ Welsh Audio Translation (Auto-Play)
    
    ## âœ¨ New Features:
    - **ğŸµ Auto-play uploaded audio** - Plays immediately after upload
    - **ğŸ“ Streaming text** - See translation text as soon as it's ready
    - **ğŸ”Š Auto-play translation** - Welsh audio plays automatically when ready
    - **âš¡ Real-time progress** - Watch each processing step
    - **ğŸš€ GPU-accelerated** - Fast inference
    
    ## ğŸ“– How to Use:
    1. Upload your English audio file â†’ **Plays automatically**
    2. Click "Translate" â†’ **Watch progress in real-time**
    3. Translation text appears â†’ **Read immediately**
    4. Welsh audio plays â†’ **Automatic playback**
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            # File upload with auto-play
            audio_input = gr.Audio(
                sources=["upload"],
                type="filepath",
                label="ğŸ¤ Upload English Audio File",
                autoplay=True  # Auto-play on upload
            )
            
            # Translate button
            translate_btn = gr.Button(
                "ğŸ”„ Translate to Welsh", 
                variant="primary",
                size="lg"
            )
            
            # Info box
            gr.Markdown(f"""
            ### ğŸ’¡ System Info
            - **Device:** {DEVICE}
            - **Model:** Fine-tuned SeamlessM4T v2
            - **Target Language:** Welsh (Cymraeg)
            
            ### âš¡ Performance
            - **GPU:** 5-15s per file
            - **CPU:** 30-90s per file
            
            ### ğŸµ Auto-Play Features
            - âœ… Original audio plays on upload
            - âœ… Translation streams as generated
            - âœ… Welsh audio plays automatically
            - âœ… Real-time progress updates
            
            ### ğŸ“ Supported Formats
            WAV, MP3, FLAC, OGG, M4A
            """)
        
        with gr.Column(scale=1):
            # Results text (updates in real-time)
            result_text = gr.Textbox(
                label="ğŸ“ Translation Results (Live Updates)",
                lines=15,
                max_lines=25
            )
            
            # Original audio player (auto-plays)
            original_audio = gr.Audio(
                label="ğŸ§ Original English Audio",
                type="numpy",
                autoplay=True  # Auto-play original
            )
            
            # Translated audio player (auto-plays)
            translated_audio = gr.Audio(
                label="ğŸ”Š Welsh Translation Audio",
                type="numpy",
                autoplay=True  # Auto-play translation
            )
    
    gr.Markdown("""
    ---
    ## ğŸ¬ What Happens When You Upload
    
    ### Timeline:
    1. **ğŸ“¤ Upload audio** â†’ Original plays automatically
    2. **âš™ï¸ Click "Translate"** â†’ Processing begins
    3. **ğŸ“Š Status updates** â†’ See progress in real-time:
       - âœ… Audio loaded
       - âœ… Audio processed
       - ğŸ“ Welsh text appears
       - ğŸ”Š Welsh audio generating...
    4. **ğŸµ Translation ready** â†’ Welsh audio plays automatically
    
    ### You'll See:
    - â±ï¸ Processing times for each step
    - ğŸ“ Welsh translation text (as soon as ready)
    - ğŸ“Š Statistics and verification
    - ğŸ”Š Auto-playing audio players
    
    ---
    ## ğŸ”§ Technical Details
    
    ### Streaming Architecture:
    ```python
    # Generator function yields intermediate results
    def translate_audio_file(audio_file):
        # Yield 1: Original audio (auto-plays)
        yield status, original_audio, None
        
        # Yield 2: Processing update
        yield status, original_audio, None
        
        # Yield 3: Text ready
        yield status_with_text, original_audio, None
        
        # Yield 4: Final with translation audio (auto-plays)
        yield final_status, original_audio, translated_audio
    ```
    
    ### Auto-Play Configuration:
    ```python
    # All audio components set to auto-play
    audio_input = gr.Audio(autoplay=True)      # Original
    original_audio = gr.Audio(autoplay=True)   # Playback
    translated_audio = gr.Audio(autoplay=True) # Translation
    ```
    
    ### Language Forcing:
    ```python
    # Guaranteed Welsh output
    outputs = model.generate(
        **inputs,
        tgt_lang="cym",
        forced_bos_token_id=256018,  # Welsh token
        num_beams=5
    )
    ```
    
    ---
    ## ğŸ“Š Expected User Experience
    
    ### Upload Audio:
    - File uploads
    - Audio plays immediately
    - Ready to translate
    
    ### Click "Translate":
    - Status: "ğŸ”„ Processing..."
    - Status: "âœ… Processed, generating text..."
    - **Text appears**: "Helo, sut ydych chi?"
    - Status: "ğŸ”„ Generating speech..."
    - **Welsh audio plays automatically**
    - Status: "âœ… Complete!"
    
    ### Total Experience Time:
    - Upload: Instant
    - Original playback: Automatic
    - Translation: 5-15s (GPU) or 30-90s (CPU)
    - Welsh playback: Automatic
    
    ---
    ## ğŸš€ Performance Optimization
    
    ### Already Applied:
    - âœ… GPU acceleration (CUDA)
    - âœ… FP16 precision
    - âœ… CUDNN benchmarking
    - âœ… TF32 matrix operations
    - âœ… Streaming updates (no blocking)
    - âœ… Memory management
    
    ### For Fastest Results:
    1. **Use GPU** (3-5x faster)
    2. **Use Google Colab T4** (free, 15GB VRAM)
    3. **Shorter audio** (< 30s recommended)
    
    ---
    ## ğŸ¯ Tips for Best Experience
    
    ### Audio Quality:
    - Clear speech (minimal background noise)
    - Standard speaking pace
    - Good microphone quality
    
    ### File Size:
    - Under 1 minute: 5-15s processing
    - 1-3 minutes: 15-45s processing
    - Over 3 minutes: May take longer
    
    ### Verification:
    Look for Welsh words like:
    **yn**, **y**, **mae**, **dw**, **chi**, **sut**, **ydych**, **rwy**, **dysgu**, **Cymraeg**
    """)
    
    # Connect button to streaming function
    translate_btn.click(
        fn=translate_audio_file,
        inputs=audio_input,
        outputs=[result_text, original_audio, translated_audio],
        show_progress=True  # Show progress bar
    )
    
    # Example files
    gr.Examples(
        examples=[
            ["audio.wav"],  # Add your example files here
        ],
        inputs=audio_input,
        label="ğŸ“‚ Example Files (Click to load)"
    )

print("ğŸŒ Launching web interface with auto-play...")

# Launch app
app.launch(
    share=True,
    inbrowser=True,
    server_name="0.0.0.0",
    server_port=7860
)