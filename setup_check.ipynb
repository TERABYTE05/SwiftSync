{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# S2S"
      ],
      "metadata": {
        "id": "_1LAomZehPR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "ideqH6XmeekM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "import whisper\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json"
      ],
      "metadata": {
        "id": "-ShMINe8S_nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare dataset"
      ],
      "metadata": {
        "id": "Rt40LE9EehiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng_train_df = pd.read_csv(os.path.join(extract_directory_src, 'train.tsv'), sep = '\\t', header = None)\n",
        "eng_train_df.columns = ['audio', 'sentence']\n",
        "eng_train_df.head(), len(eng_train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWKsCxDajkAJ",
        "outputId": "e1bf9904-943a-449b-8b05-1c0ae58a8663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                          audio                                      sentence\n",
              " 0  common_voice_cy_17301829.mp3                            i need soap please\n",
              " 1  common_voice_cy_17301830.mp3                       what time will he start\n",
              " 2  common_voice_cy_17301835.mp3                             where do you live\n",
              " 3  common_voice_cy_17301844.mp3  ifan huw dafydd iwan llwyd john pierce jones\n",
              " 4  common_voice_cy_17301845.mp3                                 what is islam,\n",
              " 1241)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_parallel_dataset(eng_df : pd.DataFrame, eng_audio_path : str, cy_audio_path : str) -> List:\n",
        "  \"\"\"Make parallel dataset from source and target dataset\"\"\"\n",
        "\n",
        "  # 1. Make path for audio and transcripts\n",
        "  eng_clips = os.path.join(eng_audio_path, 'train')\n",
        "  cy_clips = os.path.join(cy_audio_path, 'clips')\n",
        "\n",
        "  # 2. Load transcript for welsh\n",
        "  cy_transcript = os.path.join(cy_audio_path, 'train.tsv')\n",
        "  metadata = {}\n",
        "\n",
        "  if not os.path.exists(cy_transcript):\n",
        "    cy_transcript = os.path.join(cy_audio_path, 'validated.tsv')\n",
        "\n",
        "  welsh_df = pd.read_csv(cy_transcript, sep = '\\t')\n",
        "  for _, row in welsh_df.iterrows():\n",
        "    metadata[row['path']] = row['sentence']\n",
        "  print(f'Loaded {len(metadata)} welsh audio + transcripts')\n",
        "\n",
        "  # 3. Create parallel dataset\n",
        "  parallel_dataset = []\n",
        "  for i, row in eng_df.head(20).iterrows():\n",
        "\n",
        "    ## Load welsh audio\n",
        "    cy_audio_name = row['audio']\n",
        "    cy_audio_path_file = os.path.join(cy_clips, cy_audio_name)\n",
        "    if not os.path.exists(cy_audio_path_file):\n",
        "      continue\n",
        "    y_cy, sr_cy = librosa.load(cy_audio_path_file, sr = None)\n",
        "    y_cy = librosa.resample(y_cy, orig_sr=sr_cy, target_sr=16000)\n",
        "    cy_text = metadata.get(cy_audio_name, '')\n",
        "\n",
        "    ## Load english audio\n",
        "    eng_audio_name = f'{cy_audio_name}.wav'\n",
        "    eng_audio_path_file = os.path.join(eng_clips, eng_audio_name)\n",
        "\n",
        "    if not os.path.exists(eng_audio_path_file):\n",
        "      continue\n",
        "\n",
        "    y_en, sr_en = librosa.load(eng_audio_path_file, sr = None)\n",
        "    y_en = librosa.resample(y_en, orig_sr=sr_en, target_sr=16000)\n",
        "    eng_text = row.get('sentence', '')\n",
        "\n",
        "    ## Make parallel dataset\n",
        "    parallel_dataset.append({\n",
        "      'id' : i,\n",
        "      'eng_audio' : y_en,\n",
        "      'eng_transcript' : eng_text,\n",
        "      'cy_audio' : y_cy,\n",
        "      'cy_transcript' : cy_text,\n",
        "      'src' : eng_audio_name,\n",
        "      'tgt' : cy_audio_name\n",
        "    })\n",
        "\n",
        "  print(f'Loaded {len(parallel_dataset)} parallel pairs')\n",
        "  return parallel_dataset\n"
      ],
      "metadata": {
        "id": "fEh-wk3RkncJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_dataset = make_parallel_dataset(eng_train_df, extract_directory_src, audio_path_tgt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_bfEw0Jx2yd",
        "outputId": "9c602bb6-3321-4e59-c5d1-63fa75145a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8014 welsh audio + transcripts\n",
            "Loaded 20 parallel pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_dataset[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqvaoqNCyGfy",
        "outputId": "e508e85c-4c73-4808-e8b0-c037d884f774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 0,\n",
              "  'eng_audio': array([ 8.3589659e-04,  1.1703324e-03,  1.1525394e-03, ...,\n",
              "          5.8207661e-11, -5.8207661e-11,  1.1641532e-10], dtype=float32),\n",
              "  'eng_transcript': 'i need soap please',\n",
              "  'cy_audio': array([-1.2732926e-11, -4.5474735e-12,  1.4097168e-11, ...,\n",
              "         -4.6382229e-07,  5.4650445e-07,  9.3370363e-07], dtype=float32),\n",
              "  'cy_transcript': '',\n",
              "  'src': 'common_voice_cy_17301829.mp3.wav',\n",
              "  'tgt': 'common_voice_cy_17301829.mp3'},\n",
              " {'id': 1,\n",
              "  'eng_audio': array([ 7.5494684e-04,  1.0414989e-03,  1.0550146e-03, ...,\n",
              "          1.2098171e-06, -1.1129887e-06,  1.1659868e-06], dtype=float32),\n",
              "  'eng_transcript': 'what time will he start',\n",
              "  'cy_audio': array([-6.8212103e-12, -2.7284841e-12,  2.7284841e-12, ...,\n",
              "         -8.0095524e-06,  5.0972621e-06, -9.4979041e-06], dtype=float32),\n",
              "  'cy_transcript': '',\n",
              "  'src': 'common_voice_cy_17301830.mp3.wav',\n",
              "  'tgt': 'common_voice_cy_17301830.mp3'},\n",
              " {'id': 2,\n",
              "  'eng_audio': array([ 1.0495777e-03,  1.1955972e-03,  1.1615595e-03, ...,\n",
              "         -3.5390258e-08,  1.5433761e-07, -3.5052653e-07], dtype=float32),\n",
              "  'eng_transcript': 'where do you live',\n",
              "  'cy_audio': array([ 5.4569682e-12,  7.2759576e-12,  4.0927262e-12, ...,\n",
              "         -1.3523550e-06, -6.7896617e-06, -6.5718377e-06], dtype=float32),\n",
              "  'cy_transcript': '',\n",
              "  'src': 'common_voice_cy_17301835.mp3.wav',\n",
              "  'tgt': 'common_voice_cy_17301835.mp3'}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Audio prepocessing (making chunks)"
      ],
      "metadata": {
        "id": "Z13aj2ydeodW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create audio chunks\n",
        "\n",
        "@dataclass\n",
        "class AudioChunk:\n",
        "    \"\"\"Single audio chunk with metadata\"\"\"\n",
        "    audio: np.ndarray\n",
        "    chunk_id: int\n",
        "    start_time: float\n",
        "    end_time: float\n",
        "    sampling_rate: int = 16000\n",
        "\n",
        "class AudioChunker:\n",
        "    \"\"\"\n",
        "    Chunks audio into overlapping windows for streaming simulation with chunk duration of 2s and hop duration of 0.5s (overlap between chunks)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chunk_duration=2.0, hop_duration=0.5, sr=16000):\n",
        "        self.chunk_duration = chunk_duration\n",
        "        self.hop_duration = hop_duration\n",
        "        self.sr = sr\n",
        "        self.chunk_samples = int(chunk_duration * sr)\n",
        "        self.hop_samples = int(hop_duration * sr)\n",
        "\n",
        "    def create_chunks(self, audio: np.ndarray) -> List[AudioChunk]:\n",
        "        \"\"\"Create overlapping chunks from audio array\"\"\"\n",
        "        chunks = []\n",
        "        chunk_id = 0\n",
        "\n",
        "        for start in range(0, len(audio), self.hop_samples):\n",
        "            end = start + self.chunk_samples\n",
        "\n",
        "            if start >= len(audio):\n",
        "                break\n",
        "\n",
        "            # Extract chunk\n",
        "            chunk_audio = audio[start:end]\n",
        "\n",
        "            # Pad last chunk if needed\n",
        "            if len(chunk_audio) < self.chunk_samples:\n",
        "                chunk_audio = np.pad(chunk_audio, (0, self.chunk_samples - len(chunk_audio)))\n",
        "\n",
        "            chunks.append(AudioChunk(\n",
        "                audio=chunk_audio,\n",
        "                chunk_id=chunk_id,\n",
        "                start_time=start / self.sr,\n",
        "                end_time=min(end, len(audio)) / self.sr,\n",
        "                sampling_rate=self.sr\n",
        "            ))\n",
        "            chunk_id += 1\n",
        "\n",
        "        return chunks"
      ],
      "metadata": {
        "id": "WkNoFPMi1ozl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize chunker\n",
        "chunker = AudioChunker(chunk_duration=2.0, hop_duration=0.5, sr=16000)"
      ],
      "metadata": {
        "id": "I3raqoNL1tUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating context buffer\n",
        "class ContextBuffer:\n",
        "    \"\"\"\n",
        "    Maintains recent chunks for context used during training to simulate streaming with context.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_chunks=4):\n",
        "        self.max_chunks = max_chunks\n",
        "        self.buffer = deque(maxlen=max_chunks)\n",
        "\n",
        "    def add(self, chunk_audio: np.ndarray):\n",
        "        \"\"\"Add chunk to buffer\"\"\"\n",
        "        self.buffer.append(chunk_audio)\n",
        "\n",
        "    def get_context(self) -> np.ndarray:\n",
        "        \"\"\"Get concatenated audio from buffer\"\"\"\n",
        "        if len(self.buffer) == 0:\n",
        "            return None\n",
        "        return np.concatenate(list(self.buffer))\n",
        "\n",
        "    def is_ready(self) -> bool:\n",
        "        \"\"\"Check if buffer has minimum context\"\"\"\n",
        "        return len(self.buffer) >= 2  # Need at least 2 chunks\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear buffer\"\"\"\n",
        "        self.buffer.clear()"
      ],
      "metadata": {
        "id": "j-SJleq72Bp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset with chunking\n",
        "class ChunkedS2STDataset(Dataset):\n",
        "    \"\"\"\n",
        "    1. Loads parallel English to Welsh data\n",
        "    2. Chunks audio into 2s windows\n",
        "    3. Provides context buffer data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, parallel_data, chunker, max_chunks_per_sample=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            parallel_data: List of parallel pairs from make_parallel_dataset()\n",
        "            chunker: AudioChunker instance\n",
        "            max_chunks_per_sample: Max chunks to use per audio (None = all)\n",
        "        \"\"\"\n",
        "        self.parallel_data = parallel_data\n",
        "        self.chunker = chunker\n",
        "        self.max_chunks_per_sample = max_chunks_per_sample\n",
        "\n",
        "        # Pre-compute all chunks for faster training\n",
        "        self.chunked_samples = []\n",
        "\n",
        "        for sample in parallel_data:\n",
        "            # Chunk source (English) audio\n",
        "            source_chunks = chunker.create_chunks(sample['eng_audio'])\n",
        "\n",
        "            # Chunk target (Welsh) audio\n",
        "            target_chunks = chunker.create_chunks(sample['cy_audio'])\n",
        "\n",
        "            # Limit number of chunks if specified\n",
        "            if max_chunks_per_sample:\n",
        "                source_chunks = source_chunks[:max_chunks_per_sample]\n",
        "                target_chunks = target_chunks[:max_chunks_per_sample]\n",
        "\n",
        "            self.chunked_samples.append({\n",
        "                'id': sample['id'],\n",
        "                'source_chunks': source_chunks,      # English chunks\n",
        "                'target_chunks': target_chunks,      # Welsh chunks\n",
        "                'source_text': sample['eng_transcript'],  # English text\n",
        "                'target_text': sample['cy_transcript'],   # Welsh text\n",
        "                'source_file': sample['src'],\n",
        "                'target_file': sample['tgt']\n",
        "            })\n",
        "\n",
        "        print(f\"Pre-computed chunks for {len(self.chunked_samples)} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.chunked_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns one training sample with:\n",
        "        1. Source audio chunks (English)\n",
        "        2. Target audio chunks (Welsh)\n",
        "        3. Source text (English)\n",
        "        4. Target text (Welsh)\n",
        "        \"\"\"\n",
        "        sample = self.chunked_samples[idx]\n",
        "\n",
        "        return {\n",
        "            'id': sample['id'],\n",
        "            'source_chunks': sample['source_chunks'],\n",
        "            'target_chunks': sample['target_chunks'],\n",
        "            'source_text': sample['source_text'],\n",
        "            'target_text': sample['target_text'],\n",
        "            'num_source_chunks': len(sample['source_chunks']),\n",
        "            'num_target_chunks': len(sample['target_chunks'])\n",
        "        }"
      ],
      "metadata": {
        "id": "Ng9phTXV2jIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "train_dataset = ChunkedS2STDataset(parallel_data=parallel_dataset, chunker=chunker, max_chunks_per_sample=10)\n",
        "print(f'Training dataset created with {len(train_dataset)} samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJt84-WQ2vlY",
        "outputId": "90dd9e84-92f4-4225-e665-0520b868a992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-computed chunks for 20 samples\n",
            "Training dataset created with 20 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Eci8DRq3BHQ",
        "outputId": "c4dedd28-151e-462e-989a-e80711f9da2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'source_chunks': [AudioChunk(audio=array([0.0008359 , 0.00117033, 0.00115254, ..., 0.        , 0.        ,\n",
              "         0.        ], dtype=float32), chunk_id=0, start_time=0.0, end_time=1.3375, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([-0.01307321,  0.02549626, -0.01774494, ...,  0.        ,\n",
              "          0.        ,  0.        ], dtype=float32), chunk_id=1, start_time=0.5, end_time=1.3375, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([0.0122569 , 0.01207083, 0.01178565, ..., 0.        , 0.        ,\n",
              "         0.        ], dtype=float32), chunk_id=2, start_time=1.0, end_time=1.3375, sampling_rate=16000)],\n",
              " 'target_chunks': [AudioChunk(audio=array([-1.2732926e-11, -4.5474735e-12,  1.4097168e-11, ...,\n",
              "          8.1197126e-04,  7.2016800e-04,  5.6324806e-04], dtype=float32), chunk_id=0, start_time=0.0, end_time=2.0, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([1.0184628e-04, 1.8466072e-04, 2.0494346e-05, ..., 7.7348419e-02,\n",
              "         7.1228340e-02, 6.6100359e-02], dtype=float32), chunk_id=1, start_time=0.5, end_time=2.5, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([-0.01828827, -0.01359342, -0.0090191 , ...,  0.0066063 ,\n",
              "          0.00319962,  0.00067709], dtype=float32), chunk_id=2, start_time=1.0, end_time=3.0, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([ 6.0557015e-04, -2.4815986e-02,  2.3112537e-02, ...,\n",
              "         -1.9095340e-05,  2.6182450e-05,  4.5247139e-05], dtype=float32), chunk_id=3, start_time=1.5, end_time=3.5, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([0.0003061 , 0.00031813, 0.00025119, ..., 0.        , 0.        ,\n",
              "         0.        ], dtype=float32), chunk_id=4, start_time=2.0, end_time=3.936, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([0.05760067, 0.0411622 , 0.02322594, ..., 0.        , 0.        ,\n",
              "         0.        ], dtype=float32), chunk_id=5, start_time=2.5, end_time=3.936, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([-0.00099901, -0.00139255, -0.00198583, ...,  0.        ,\n",
              "          0.        ,  0.        ], dtype=float32), chunk_id=6, start_time=3.0, end_time=3.936, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([-1.1602486e-05,  8.1410733e-05,  8.3009349e-05, ...,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32), chunk_id=7, start_time=3.5, end_time=3.936, sampling_rate=16000)],\n",
              " 'source_text': 'i need soap please',\n",
              " 'target_text': '',\n",
              " 'num_source_chunks': 3,\n",
              " 'num_target_chunks': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create collate function\n",
        "def collate_chunked_batch(batch):\n",
        "    \"\"\"Handles variable-length chunks per sample\"\"\"\n",
        "\n",
        "    if len(batch) == 1:\n",
        "        return batch[0]\n",
        "\n",
        "    # For larger batches, padding will be applied. So for now, we use batch_size=1\n",
        "    raise NotImplementedError('Batch size > 1 not yet supported')"
      ],
      "metadata": {
        "id": "RYCHNEMU3s_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaderder\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_chunked_batch, num_workers=0)\n",
        "\n",
        "# Get one batch\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ],
      "metadata": {
        "id": "NoAyk7vkPPF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3489f253-c54a-4643-9d00-8b14641f6b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 17,\n",
              " 'source_chunks': [AudioChunk(audio=array([0.00225219, 0.00283589, 0.00261182, ..., 0.00253015, 0.00149526,\n",
              "         0.00111748], dtype=float32), chunk_id=0, start_time=0.0, end_time=2.0, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([0.02935768, 0.0112407 , 0.022212  , ..., 0.        , 0.        ,\n",
              "         0.        ], dtype=float32), chunk_id=1, start_time=0.5, end_time=2.1125, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([0.02652753, 0.03057722, 0.02021658, ..., 0.        , 0.        ,\n",
              "         0.        ], dtype=float32), chunk_id=2, start_time=1.0, end_time=2.1125, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([0.09374263, 0.09572086, 0.10089844, ..., 0.        , 0.        ,\n",
              "         0.        ], dtype=float32), chunk_id=3, start_time=1.5, end_time=2.1125, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([0.00172371, 0.00146554, 0.0012197 , ..., 0.        , 0.        ,\n",
              "         0.        ], dtype=float32), chunk_id=4, start_time=2.0, end_time=2.1125, sampling_rate=16000)],\n",
              " 'target_chunks': [AudioChunk(audio=array([0.        , 0.        , 0.        , ..., 0.0006081 , 0.0005852 ,\n",
              "         0.00103519], dtype=float32), chunk_id=0, start_time=0.0, end_time=2.0, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([0.00011966, 0.00018889, 0.00022065, ..., 0.06350888, 0.06953992,\n",
              "         0.07323902], dtype=float32), chunk_id=1, start_time=0.5, end_time=2.5, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([-0.07828054, -0.09531087, -0.09712976, ..., -0.1528481 ,\n",
              "         -0.12358847, -0.08467998], dtype=float32), chunk_id=2, start_time=1.0, end_time=3.0, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([ 0.00801671, -0.01833423, -0.03879577, ...,  0.01171823,\n",
              "          0.0234466 ,  0.0334669 ], dtype=float32), chunk_id=3, start_time=1.5, end_time=3.5, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([ 0.00024576,  0.00035191,  0.00037453, ..., -0.00115179,\n",
              "         -0.00228534, -0.00124859], dtype=float32), chunk_id=4, start_time=2.0, end_time=4.0, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([ 0.074742  ,  0.07560743,  0.07595278, ..., -0.00033516,\n",
              "         -0.00041985, -0.0003973 ], dtype=float32), chunk_id=5, start_time=2.5, end_time=4.5, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([-0.03758004,  0.00686571,  0.04516538, ...,  0.        ,\n",
              "          0.        ,  0.        ], dtype=float32), chunk_id=6, start_time=3.0, end_time=4.992, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([0.04157588, 0.04697591, 0.04951357, ..., 0.        , 0.        ,\n",
              "         0.        ], dtype=float32), chunk_id=7, start_time=3.5, end_time=4.992, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([0.00121346, 0.00236233, 0.00190911, ..., 0.        , 0.        ,\n",
              "         0.        ], dtype=float32), chunk_id=8, start_time=4.0, end_time=4.992, sampling_rate=16000),\n",
              "  AudioChunk(audio=array([-8.2543847e-05,  2.7243985e-04, -2.3605833e-04, ...,\n",
              "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32), chunk_id=9, start_time=4.5, end_time=4.992, sampling_rate=16000)],\n",
              " 'source_text': \"to start i'd like a green salad\",\n",
              " 'target_text': '',\n",
              " 'num_source_chunks': 5,\n",
              " 'num_target_chunks': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8neCvgCpo-PB",
        "dgyqYLCt47ZS"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}